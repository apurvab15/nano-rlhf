=== Starting run for dataset size 200000 ===
Timestamp: Tue Oct 21 09:02:12 PM CDT 2025
----------------------------------------------
>>> Training...
tokens per iteration will be: 1,024
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/nanoGPT_venv/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  self.setter(val)
found vocab_size = 82 (inside data/taylor_swift_lyrics_200000/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 10,697,472 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/nanoGPT_venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: NVIDIA GeForce RTX 2080 Ti does not support bfloat16 compilation natively, skipping
  warnings.warn(
step 0: train loss 4.4836, val loss 4.4828
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/nanoGPT_venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: NVIDIA GeForce RTX 2080 Ti does not support bfloat16 compilation natively, skipping
  warnings.warn(
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/nanoGPT_venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: NVIDIA GeForce RTX 2080 Ti does not support bfloat16 compilation natively, skipping
  warnings.warn(
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/nanoGPT_venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: NVIDIA GeForce RTX 2080 Ti does not support bfloat16 compilation natively, skipping
  warnings.warn(
iter 0: loss 4.4758, time 11775.75ms, mfu -100.00%
step 250: train loss 2.1423, val loss 2.1530
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 250: loss 2.1842, time 32422.82ms, mfu 0.00%
step 500: train loss 1.8078, val loss 1.8436
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 500: loss 1.7693, time 30210.13ms, mfu 0.00%
step 750: train loss 1.6660, val loss 1.6920
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 750: loss 1.7413, time 33173.59ms, mfu 0.00%
step 1000: train loss 1.5574, val loss 1.6282
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 1000: loss 1.6763, time 33441.41ms, mfu 0.00%
step 1250: train loss 1.4817, val loss 1.5643
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 1250: loss 1.6141, time 31911.41ms, mfu 0.00%
step 1500: train loss 1.4043, val loss 1.5245
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 1500: loss 1.5084, time 32803.75ms, mfu 0.00%
step 1750: train loss 1.3649, val loss 1.5056
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 1750: loss 1.4818, time 33687.30ms, mfu 0.00%
step 2000: train loss 1.3356, val loss 1.4870
saving checkpoint to out/taylor_swift_lyrics/taylor_swift_lyrics_200000
iter 2000: loss 1.4331, time 37693.73ms, mfu 0.00%
>>> Generating samples...
/afs/cs.wisc.edu/u/a/p/apurvaashok/foundational_model/nanoGPT/nanoGPT_venv/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  self.setter(val)
>>> Evaluating metrics...

===== Evaluation Metrics =====
KL Divergence (3-gram): 3.904912
Distinct-1: 0.0115
Distinct-2: 0.0814
==============================

Completed run for dataset size: 200000
Timestamp: Tue Oct 21 09:08:59 PM CDT 2025
==============================================
